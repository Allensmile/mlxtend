{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sebastian Raschka, 2015  \n",
    "`mlxtend`, a library of extension and helper modules for Python's data analysis and machine learning libraries\n",
    "\n",
    "- GitHub repository: https://github.com/rasbt/mlxtend\n",
    "- Documentation: http://rasbt.github.io/mlxtend/\n",
    "\n",
    "View this page in [jupyter nbviewer](http://nbviewer.ipython.org/github/rasbt/mlxtend/blob/master/docs/sources/_ipynb_templates/regressor/linear_regression.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Raschka \n",
      "last updated: 2016-11-03 \n",
      "\n",
      "CPython 3.5.2\n",
      "IPython 5.1.0\n",
      "\n",
      "matplotlib 1.5.3\n",
      "numpy 1.11.2\n",
      "scipy 0.18.1\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Sebastian Raschka' -u -d -v -p matplotlib,numpy,scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of a Perceptron learning algorithm for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> from mlxtend.classifier import Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind this \"thresholded\" perceptron was to mimic how a single neuron in the brain works: It either \"fires\" or not. \n",
    "A perceptron receives multiple input signals, and if the sum of the input signals exceed a certain threshold it either returns a signal or remains \"silent\" otherwise. What made this a \"machine learning\" algorithm was Frank Rosenblatt's idea of the perceptron learning rule: The perceptron algorithm is about learning the weights for the input signals in order to draw linear decision boundary that allows us to discriminate between the two linearly separable classes +1 and -1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./Perceptron_files/perceptron_schematic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive deeper into the algorithm(s) for learning the weights of the perceptron classifier, let us take a brief look at the basic notation. In the following sections, we will label the *positive* and *negative* class in our binary classification setting as \"1\" and \"-1\", respectively. Next, we define an activation function $g(\\mathbf{z})$ that takes a linear combination of the input values $\\mathbf{x}$ and weights $\\mathbf{w}$ as input ($\\mathbf{z} = w_1x_{1} + \\dots + w_mx_{m}$), and if $g(\\mathbf{z})$ is greater than a defined threshold $\\theta$ we predict 1 and -1 otherwise; in this case, this activation function $g$ is a simple \"unit step function,\" which is sometimes also called \"Heaviside step function.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    " g(z) =\\begin{cases}\n",
    "    1 & \\text{if $z \\ge \\theta$}\\\\\n",
    "    -1 & \\text{otherwise}.\n",
    "  \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "\n",
    "$$z =  w_1x_{1} + \\dots + w_mx_{m} = \\sum_{j=1}^{m} x_{j}w_{j} \\\\ = \\mathbf{w}^T\\mathbf{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{w}$ is the feature vector, and $\\mathbf{x}$ is an $m$-dimensional sample from the training dataset:\n",
    "\n",
    "$$ \n",
    "\\mathbf{w} = \\begin{bmatrix}\n",
    "    w_{1}  \\\\\n",
    "    \\vdots \\\\\n",
    "    w_{m}\n",
    "\\end{bmatrix}\n",
    "\\quad  \\mathbf{x} = \\begin{bmatrix}\n",
    "    x_{1}  \\\\\n",
    "    \\vdots \\\\\n",
    "    x_{m}\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to simplify the notation, we bring $\\theta$ to the left side of the equation and define $w_0 = -\\theta  \\text{ and } x_0=1$ \n",
    "\n",
    "so that \n",
    "\n",
    "$$\n",
    " g({z}) =\\begin{cases}\n",
    "    1 & \\text{if $z \\ge 0$}\\\\\n",
    "    -1 & \\text{otherwise}.\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "\n",
    "$$z = w_0x_{0} + w_1x_{1} + \\dots + w_mx_{m} = \\sum_{j=0}^{m} x_{j}w_{j} \\\\ = \\mathbf{w}^T\\mathbf{x}.$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rosenblatt's initial perceptron rule is fairly simple and can be summarized by the following steps: \n",
    "\n",
    "1. Initialize the weights to 0 or small random numbers.\n",
    "2. For each training sample $\\mathbf{x^{(i)}}$:\n",
    "    2. Calculate the *output* value.\n",
    "    2. Update the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output value is the class label predicted by the unit step function that we defined earlier (output $=g(\\mathbf{z})$) and the weight update can be written more formally as  $w_j := w_j + \\Delta w_j$.\n",
    "\n",
    "The value for updating the weights at each increment is calculated by the learning rule\n",
    "\n",
    "$\\Delta w_j = \\eta \\; (\\text{target}^{(i)} - \\text{output}^{(i)})\\;x^{(i)}_{j}$\n",
    "\n",
    "where $\\eta$ is the learning rate (a constant between 0.0 and 1.0), \"target\" is the true class label, and the \"output\" is the predicted class label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aIt is important to note that all weights in the weight vector are being updated simultaneously. Concretely, for a 2-dimensional dataset, we would write the update as:\n",
    "\n",
    "$\\Delta w_0 = \\eta(\\text{target}^{(i)} - \\text{output}^{(i)})$  \n",
    "$\\Delta w_1 = \\eta(\\text{target}^{(i)} - \\text{output}^{(i)})\\;x^{(i)}_{1}$  \n",
    "$\\Delta w_2 = \\eta(\\text{target}^{(i)} - \\text{output}^{(i)})\\;x^{(i)}_{2}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we implement the perceptron rule in Python, let us make a simple thought experiment to illustrate how beautifully simple this learning rule really is. In the two scenarios where the perceptron predicts the class label correctly, the weights remain unchanged:\n",
    "\n",
    "- $\\Delta w_j = \\eta(-1^{(i)} - -1^{(i)})\\;x^{(i)}_{j} = 0$ \n",
    "- $\\Delta w_j = \\eta(1^{(i)} - 1^{(i)})\\;x^{(i)}_{j} = 0$ \n",
    "\n",
    "However, in case of a wrong prediction, the weights are being \"pushed\" towards the direction of the positive or negative target class, respectively:\n",
    "\n",
    "- $\\Delta w_j = \\eta(1^{(i)} - -1^{(i)})\\;x^{(i)}_{j} = \\eta(2)\\;x^{(i)}_{j}$ \n",
    "- $\\Delta w_j = \\eta(-1^{(i)} - 1^{(i)})\\;x^{(i)}_{j} = \\eta(-2)\\;x^{(i)}_{j}$ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that the convergence of the perceptron is only guaranteed if the two classes are linearly separable. If the two classes can't be separated by a linear decision boundary, we can set a maximum number of passes over the training dataset (\"epochs\") and/or a threshold for the number of tolerated misclassifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- F. Rosenblatt. The perceptron, a perceiving and recognizing automaton Project Para. Cornell Aeronautical Laboratory, 1957."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1 - Classification of Iris Flowers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "test_sth() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-cb80cd56a3e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtest_sth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: test_sth() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "test_1(**kwargs):\n",
    "    test_sth(kwargs)\n",
    "\n",
    "def test_sth(**kwargs):\n",
    "    print(kwargs)\n",
    "    \n",
    "test_sth(a=1, b=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 5/5 | Elapsed: 00:00:00 | ETA: 00:00:00"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "plot_decision_regions() takes 0 positional arguments but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d18b5e4bacd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mppn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mplot_decision_regions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mppn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Perceptron - Rosenblatt Perceptron Rule'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: plot_decision_regions() takes 0 positional arguments but 2 were given"
     ]
    }
   ],
   "source": [
    "from mlxtend.data import iris_data\n",
    "from mlxtend.evaluate import plot_decision_regions\n",
    "from mlxtend.classifier import Perceptron\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loading Data\n",
    "\n",
    "X, y = iris_data()\n",
    "X = X[:, [0, 3]] # sepal length and petal width\n",
    "X = X[0:100] # class 0 and class 1\n",
    "y = y[0:100] # class 0 and class 1\n",
    "\n",
    "# standardize\n",
    "X[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\n",
    "X[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n",
    "\n",
    "\n",
    "# Rosenblatt Perceptron\n",
    "\n",
    "ppn = Perceptron(epochs=5, \n",
    "                 eta=0.05, \n",
    "                 random_seed=0,\n",
    "                 print_progress=3)\n",
    "ppn.fit(X, y)\n",
    "\n",
    "plot_decision_regions(X, y, clf=ppn)\n",
    "plt.title('Perceptron - Rosenblatt Perceptron Rule')\n",
    "plt.show()\n",
    "\n",
    "print('Bias & Weights: %s' % ppn.w_)\n",
    "\n",
    "plt.plot(range(len(ppn.cost_)), ppn.cost_)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Missclassifications')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/Transcend/code/mlxtend/docs/sources/user_guide/classifier'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Perceptron\n",
      "\n",
      "*Perceptron(eta=0.1, epochs=50, random_seed=None, print_progress=0)*\n",
      "\n",
      "Perceptron classifier.\n",
      "\n",
      "    Note that this implementation of the Perceptron expects binary class labels\n",
      "    in {0, 1}.\n",
      "\n",
      "**Parameters**\n",
      "\n",
      "- `eta` : float (default: 0.1)\n",
      "\n",
      "    Learning rate (between 0.0 and 1.0)\n",
      "\n",
      "- `epochs` : int (default: 50)\n",
      "\n",
      "    Number of passes over the training dataset.\n",
      "    Prior to each epoch, the dataset is shuffled to prevent cycles.\n",
      "\n",
      "- `random_seed` : int\n",
      "\n",
      "    Random state for initializing random weights and shuffling.\n",
      "\n",
      "- `print_progress` : int (default: 0)\n",
      "\n",
      "    Prints progress in fitting to stderr.\n",
      "    0: No output\n",
      "    1: Epochs elapsed and cost\n",
      "    2: 1 plus time elapsed\n",
      "    3: 2 plus estimated time until completion\n",
      "\n",
      "**Attributes**\n",
      "\n",
      "- `w_` : 2d-array, shape={n_features, 1}\n",
      "\n",
      "    Model weights after fitting.\n",
      "\n",
      "- `b_` : 1d-array, shape={1,}\n",
      "\n",
      "    Bias unit after fitting.\n",
      "\n",
      "- `cost_` : list\n",
      "\n",
      "    Number of misclassifications in every epoch.\n",
      "\n",
      "### Methods\n",
      "\n",
      "<hr>\n",
      "\n",
      "*fit(X, y, init_params=True)*\n",
      "\n",
      "Learn model from training data.\n",
      "\n",
      "**Parameters**\n",
      "\n",
      "- `X` : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "\n",
      "    Training vectors, where n_samples is the number of samples and\n",
      "    n_features is the number of features.\n",
      "\n",
      "- `y` : array-like, shape = [n_samples]\n",
      "\n",
      "    Target values.\n",
      "\n",
      "- `init_params` : bool (default: True)\n",
      "\n",
      "    Re-initializes model parameters prior to fitting.\n",
      "    Set False to continue training with weights from\n",
      "    a previous model fitting.\n",
      "\n",
      "**Returns**\n",
      "\n",
      "- `self` : object\n",
      "\n",
      "\n",
      "<hr>\n",
      "\n",
      "*predict(X)*\n",
      "\n",
      "Predict targets from X.\n",
      "\n",
      "**Parameters**\n",
      "\n",
      "- `X` : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "\n",
      "    Training vectors, where n_samples is the number of samples and\n",
      "    n_features is the number of features.\n",
      "\n",
      "**Returns**\n",
      "\n",
      "- `target_values` : array-like, shape = [n_samples]\n",
      "\n",
      "    Predicted target values.\n",
      "\n",
      "<hr>\n",
      "\n",
      "*score(X, y)*\n",
      "\n",
      "Compute the prediction accuracy\n",
      "\n",
      "**Parameters**\n",
      "\n",
      "- `X` : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "\n",
      "    Training vectors, where n_samples is the number of samples and\n",
      "    n_features is the number of features.\n",
      "\n",
      "- `y` : array-like, shape = [n_samples]\n",
      "\n",
      "    Target values (true class labels).\n",
      "\n",
      "**Returns**\n",
      "\n",
      "- `acc` : float\n",
      "\n",
      "    The prediction accuracy as a float\n",
      "    between 0.0 and 1.0 (perfect score).\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import os\n",
    "\n",
    "filename = inspect.getframeinfo(inspect.currentframe()).filename\n",
    "path = os.path.dirname(os.path.abspath(filename))\n",
    "path = os.path.join('..', '..', )\n",
    "\n",
    "with open('../../api_modules/mlxtend.classifier/Perceptron.md', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
